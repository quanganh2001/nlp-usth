{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Customized RNN model for sequence classification\n",
        "\n",
        "Your name: Nguyen Van A\n",
        "\n",
        "Student ID: USTH001\n",
        "\n",
        "**Due: March 27, 2023 (Hard Deadline)**\n",
        "\n",
        "*This is an optional (bonus) assignment*\n",
        "\n",
        "## How to submit\n",
        "\n",
        "- Attach notebook file (.ipynb) and submit your work to Google Class Room\n",
        "- Name your file as YourName_StudentID_Assignment5.ibynb. E.g., Nguyen_Van_A_ST099834_Assignment5.ipynb\n",
        "- Write your name and student ID into this notebook\n",
        "- Copying others' assignments is strictly prohibited.\n",
        "\n",
        "## Policy\n",
        "\n",
        "- I only grade submissions which can run successfully without syntax and run-time errors\n",
        "- Please run your notebook on Google Colab or Kaggle notebook.\n",
        "- You can change epochs to 5 to save time\n"
      ],
      "metadata": {
        "id": "U94KiNBLNi_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description\n",
        "\n",
        "In this assignment, we are going to build a customized RNN model for text classification. That is the extended version for the notebook [RNN for Sequence Classification](https://colab.research.google.com/drive/1qkRmcd5PI5ISWScdD0BovDSvgYYkruVc?usp=sharing).\n",
        "\n",
        "The tasks in this assignment is as follows.\n",
        "\n",
        "- Initialize Embedding layer in the RNN model with pre-trained word embeddings\n",
        "- Modify the training loop to print out averaged loss function value after each epoch."
      ],
      "metadata": {
        "id": "NV4MB9rHurDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set random seed"
      ],
      "metadata": {
        "id": "o6dAPAEAutW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCNsWVG0u4GW",
        "outputId": "1a87d19c-6fe7-4d55-f968-43aa88b9e28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f317c331710>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download & load data"
      ],
      "metadata": {
        "id": "PpLv0rGSu4vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -f titles-en-train.labeled\n",
        "!rm -f titles-en-test.labeled\n",
        "\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/titles-en-train.labeled\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/titles-en-test.labeled"
      ],
      "metadata": {
        "id": "J0r44Yacu9WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            lb, text = line.split('\\t')\n",
        "            data.append((text,int(lb)))\n",
        "\n",
        "    return data\n",
        "\n",
        "train_data = load_data('./titles-en-train.labeled')\n",
        "test_data = load_data('./titles-en-test.labeled')\n",
        "\n",
        "train_docs, train_labels = zip(*train_data)\n",
        "test_docs, test_labels = zip(*test_data)"
      ],
      "metadata": {
        "id": "Jy3PbYn7vDJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Transformation"
      ],
      "metadata": {
        "id": "niLadkNyvGOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary\n"
      ],
      "metadata": {
        "id": "HfmaqTSOvPNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, token_to_idx=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
        "        \"\"\"\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "\n",
        "        self._idx_to_token = {idx: token\n",
        "                              for token, idx in self._token_to_idx.items()}\n",
        "\n",
        "        self.pad_index = 0\n",
        "        self.unk_index = 1\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token\n",
        "          or the UNK index if token isn't present.\n",
        "\n",
        "        Args:\n",
        "            token (str): the token to look up\n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        Notes:\n",
        "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n",
        "              for the UNK functionality\n",
        "        \"\"\"\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        \"\"\"Return the token associated with the index\n",
        "\n",
        "        Args:\n",
        "            index (int): the index to look up\n",
        "        Returns:\n",
        "            token (str): the token corresponding to the index\n",
        "        Raises:\n",
        "            KeyError: if the index is not in the Vocabulary\n",
        "        \"\"\"\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "\n",
        "    def add_token(self, token):\n",
        "        \"\"\"Update mapping dicts based on the token.\n",
        "\n",
        "        Args:\n",
        "            token (str): the item to add into the Vocabulary\n",
        "        Returns:\n",
        "            index (int): the integer corresponding to the token\n",
        "        \"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "    @classmethod\n",
        "    def build_vocab(cls, sentences):\n",
        "        \"\"\"Build vocabulary from a list of sentences\n",
        "\n",
        "        Arguments:\n",
        "        ----------\n",
        "            sentences (list): list of sentences, each sentence is a string\n",
        "\n",
        "        Return:\n",
        "        ----------\n",
        "            vocab (Vocabulary): a Vocabulary object\n",
        "        \"\"\"\n",
        "        token_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "        vocab = cls(token_to_idx)\n",
        "\n",
        "        for s in sentences:\n",
        "            for word in s.split():\n",
        "                vocab.add_token(word)\n",
        "        return vocab\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)"
      ],
      "metadata": {
        "id": "oFJaSFy7vMCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocabulary.build_vocab(train_docs)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhLikk2evRld",
        "outputId": "a59afd44-342a-48a7-9d40-ace79fef6c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Vocabulary(size=27192)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Vectorization"
      ],
      "metadata": {
        "id": "R2QEmVakvVRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def vectorize(vocab, title):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        vocab (Vocabulary)\n",
        "        title (str): the string of characters\n",
        "        max_length (int): an argument for forcing the length of index vector\n",
        "    \"\"\"\n",
        "    indices = [vocab.lookup_token(token) for token in title.split()]\n",
        "\n",
        "    return torch.tensor(indices)"
      ],
      "metadata": {
        "id": "M3iOVIFevXQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [vectorize(vocab, t) for t in train_docs]\n",
        "test_data = [vectorize(vocab, t) for t in test_docs]"
      ],
      "metadata": {
        "id": "KwGbrINhvZUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Mapping"
      ],
      "metadata": {
        "id": "4-CR6Eaxve-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label2idx = {\n",
        "    -1: 0, 1: 1\n",
        "}\n",
        "train_y = [label2idx[lb] for lb in train_labels]\n",
        "test_y = [label2idx[lb] for lb in test_labels]"
      ],
      "metadata": {
        "id": "k1Gg2YzIvfgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset class\n",
        "\n",
        "In order to put data into DataLoader, we need to implement a custom Dataset class that inherite [Dataset class](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "\n",
        "It is required to implement two functions `__len__` and `__getitem__`"
      ],
      "metadata": {
        "id": "naK37_unvgnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.sequences[index]\n",
        "        y = self.labels[index]\n",
        "\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "2h9Odv5Vvi6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(train_data, train_y)\n",
        "test_dataset = TextDataset(test_data, test_y)"
      ],
      "metadata": {
        "id": "J8PCcdzDvkjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create DataLoader\n",
        "\n",
        "We need to define function for processing batches generated by DataLoader"
      ],
      "metadata": {
        "id": "1Zi-UZZvvmsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\"Processing a batch generated by DataLoader\n",
        "\n",
        "    Arguments:\n",
        "    -----\n",
        "        batch (torch.tensor): a tensor generated by DataLoader\n",
        "    \"\"\"\n",
        "    (x, y) = zip(*batch)\n",
        "    x_lens = torch.tensor([len(x) for x in x])\n",
        "    y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    x_pad = pad_sequence(x, batch_first=True, padding_value=0)\n",
        "\n",
        "    return x_pad, x_lens, y"
      ],
      "metadata": {
        "id": "qmOiBKSbvpT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Creating Embedding Matrix from Pre-trained Glove (50 points)\n",
        "\n",
        "What you need to do is to create an embedding matrix which is a tensor with shape (vocab_size, embedidng_size). Row $i$ in the matrix is the word vector for the word of the index $i$ in the vocabulary. We are going to get those word vectors from pre-trained word vectors.\n",
        "\n",
        "We will download the pre-trained word vectors using gensim."
      ],
      "metadata": {
        "id": "BQ7s9cYxvqcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECtVPGQ5vw5i",
        "outputId": "0ad5d2e7-0610-4bdc-cc5e-b94538a6c78a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You will need to implement the following function**\n",
        "\n",
        "Note that: words in glove model are lower-cases, so to get the vector for a word, such as \"King\", we need to convert the word into lower case first. For instance\n",
        "\n",
        "```\n",
        "print( wv[\"King\".lower()] )\n",
        "```\n",
        "\n",
        "For words in the Vocabulary but not in pre-trained word vector model, we will assign random values by:\n",
        "\n",
        "```\n",
        "torch.randn(emb_dim)\n",
        "```"
      ],
      "metadata": {
        "id": "3_v55qLZ0Bsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(wv, word_to_idx, emb_dim=300):\n",
        "    \"\"\"Create embedding matrix\n",
        "\n",
        "    Args:\n",
        "        wv: word vector model loaded from gensim\n",
        "        word_to_idx (dict): Map from a word into index in the vocab\n",
        "        emb_dim (int): Embedding size\n",
        "\n",
        "    Returns:\n",
        "        embedding_matrix: a Torch tensor with size (vocab_size, emb_dim)\n",
        "    \"\"\"\n",
        "    vocab_size = len(word_to_idx)\n",
        "    embedding_matrix = torch.zeros(vocab_size, emb_dim)\n",
        "\n",
        "    #TODO: Write your code here\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "WF5MnJ_T0bo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that, create an embedding matrix"
      ],
      "metadata": {
        "id": "PknI_VBz3c4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_idx = vocab._token_to_idx\n",
        "embedding_matrix = create_embedding_matrix(wv, word_to_idx)"
      ],
      "metadata": {
        "id": "tYxbd6uf3f5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Model\n",
        "\n",
        "We just modify the model to initialize word embedding layer by pre-trained word vectors."
      ],
      "metadata": {
        "id": "FH8dfnHz2nGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_matrix, rnn_hidden_size, num_classes,\n",
        "                 num_layers=1, batch_first=True, padding_idx=0):\n",
        "\n",
        "        super(TextClassifier, self).__init__()\n",
        "\n",
        "        vocab_size, embedding_size = embedding_matrix.shape\n",
        "\n",
        "        self.emb = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n",
        "        # Use Bidirectional LSTM\n",
        "        self.rnn = nn.LSTM(input_size=embedding_size, hidden_size=rnn_hidden_size,\n",
        "                          batch_first=batch_first, num_layers=num_layers, bidirectional=True)\n",
        "        self.fc = nn.Linear(in_features=2*rnn_hidden_size, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x_in, x_lens):\n",
        "        x_embed = self.emb(x_in)\n",
        "        x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=True, enforce_sorted=False)\n",
        "        _, (hidden_state, _) = self.rnn(x_packed)\n",
        "\n",
        "        # Concatenating the final forward and backward hidden states\n",
        "        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
        "\n",
        "        logits = torch.sigmoid(self.fc(hidden))\n",
        "        return logits"
      ],
      "metadata": {
        "id": "sVIqXt8o22Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Customized Training loop (50 points)\n",
        "\n",
        "Modify the training loss to print-out averaged loss function after each epoch"
      ],
      "metadata": {
        "id": "l0Rj3pC53CwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "rnn_hidden_size = 128\n",
        "num_layers=1\n",
        "num_classes = 1\n",
        "batch_first = True\n",
        "\n",
        "model = TextClassifier(embedding_matrix,\n",
        "                       rnn_hidden_size=rnn_hidden_size,\n",
        "                       num_classes=num_classes,\n",
        "                       batch_first=batch_first, num_layers=num_layers)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 16\n",
        "\n",
        "# You can change epochs to 5 to save time\n",
        "epochs = 20\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        collate_fn=collate_batch,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    model.train()\n",
        "    train_iterator = trange(int(epochs), desc=\"Epoch\")\n",
        "\n",
        "    for epoch in train_iterator:\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for x_in, x_lens, y in train_dataloader:\n",
        "            x_in = x_in.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x_in, x_lens).squeeze()\n",
        "            loss = criterion(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #TODO: Write code to add the loss value to epoch_loss and increase num_batches\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / num_batches\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}] - Average Loss: {avg_epoch_loss}\")\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "id": "P129p8Rl3Owb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "If there are no bug in your code, you should be able to calcuate evaluation metrics on the test data."
      ],
      "metadata": {
        "id": "oL2zyXS64ViK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        collate_fn=collate_batch,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for x_in, x_lens, y in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "            x_in = x_in.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            logits = model(x_in, x_lens).squeeze()\n",
        "            _preds = (logits>0.5).type(torch.long)\n",
        "            preds += _preds.detach().cpu().numpy().tolist()\n",
        "            true_labels += y.detach().cpu().numpy().tolist()\n",
        "\n",
        "    print(metrics.classification_report(true_labels, preds))\n",
        "\n",
        "evaluate()"
      ],
      "metadata": {
        "id": "5JUpL6es4XbM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}