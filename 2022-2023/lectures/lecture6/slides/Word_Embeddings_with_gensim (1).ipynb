{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings and Applications\n",
        "\n",
        "Notebook Contents:\n",
        "- Loading word embeddings using [gensim](https://radimrehurek.com/gensim/) package\n",
        "- Using word embeddings for text classification task\n",
        "- Training a word2vec model using gensim"
      ],
      "metadata": {
        "id": "C4qEG47MDHeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "metadata": {
        "id": "tywE5kJPFqhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading pre-trained word2vec model"
      ],
      "metadata": {
        "id": "zUoqlTw3ECKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "id": "CHgemDH5FmYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf48f2d6-833c-4533-c357-04ce4fd5a06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 98.1% 369.0/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(wv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fov4PP5e7nQd",
        "outputId": "446b4d48-5340-4678-a6cb-f7cd3e0ab6c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can obtain vector representation of a word"
      ],
      "metadata": {
        "id": "hT5DXTA5Fvf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec_king = wv['king']\n",
        "vec_king.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWUyPVsuGE3D",
        "outputId": "6ab70ff1-4f9d-41a6-e9ac-f0efb5373ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate word similarity\n",
        "\n",
        "Using the function `wv.similarity`"
      ],
      "metadata": {
        "id": "Nrfoc5dTGLWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'automobile'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTPXuFW7GPlm",
        "outputId": "db72ec40-3bc7-4ee1-8042-94f512752838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'car'\t'minivan'\t0.50\n",
            "'car'\t'automobile'\t0.60\n",
            "'car'\t'bicycle'\t0.50\n",
            "'car'\t'airplane'\t0.43\n",
            "'car'\t'cereal'\t0.03\n",
            "'car'\t'communism'\t0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the 5 most similar words to “car” or “minivan”"
      ],
      "metadata": {
        "id": "qiRSPcqmIBY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXqfeOvqIDvS",
        "outputId": "59d8d03e-93d0-47cb-eed2-100ecea13b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('suv', 0.7696972489356995), ('vehicle', 0.7469112873077393), ('truck', 0.7312718629837036), ('cars', 0.7033854722976685), ('jeep', 0.6848679184913635)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which of the below does not belong in the sequence?"
      ],
      "metadata": {
        "id": "YZeTiAKxIHc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HK0as1TIN67",
        "outputId": "c0a01a62-b1cc-47d4-8840-50a887544a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Analogy\n"
      ],
      "metadata": {
        "id": "mr0oOrO7Imoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.similar_by_vector(wv['spain'] - wv['madrid'] + wv['athens'], topn=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "___wFYKdJAeO",
        "outputId": "35c3efb8-1062-4068-e814-b555ced1c260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('greece', 0.7637240886688232), ('athens', 0.7158880233764648), ('spain', 0.5469861030578613), ('greek', 0.5434280633926392), ('cyprus', 0.5079883933067322), ('bulgaria', 0.49355754256248474), ('portugal', 0.4708734154701233), ('hungary', 0.4684615135192871), ('crete', 0.4490693211555481), ('greeks', 0.4459525942802429)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.similar_by_vector(wv['king'] - wv['man'] + wv['woman'], topn=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY_zN2X2kApQ",
        "outputId": "0f6bf5e9-4cd2-488a-8d9e-1ca8a904debe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('king', 0.8065858483314514), ('queen', 0.6896163821220398), ('monarch', 0.5575490593910217), ('throne', 0.5565374493598938), ('princess', 0.5518684387207031), ('mother', 0.5142154097557068), ('daughter', 0.5133156776428223), ('kingdom', 0.5025345087051392), ('prince', 0.5017740726470947), ('elizabeth', 0.4908031225204468)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Word Embeddings for Text Classification Task"
      ],
      "metadata": {
        "id": "HMvssy27LiKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the part, You will use the [News Aggregator Data Set](https://archive.ics.uci.edu/ml/datasets/News+Aggregator) to build a model that can classify an articles into \"business\", \"science and technology\", \"entertainment\", \"health\" based on articles' titles.\n",
        "\n",
        "We have prepared train ([train.csv](https://www.dl.dropboxusercontent.com/s/rs7sqtb87m30o17/train.csv)), and test ([test.csv](https://www.dl.dropboxusercontent.com/s/fu4wa76kiwlby7u/test.csv) data sets. We used csv format to save data files. In first line of each file is the header with two columns: \"TITLE\" and \"CATEGORY\". We will use articles' titles for classification.\n",
        "\n",
        "Meaning of categories are as follows.\n",
        "\n",
        "(b = business, t = science and technology, e = entertainment, m = health)"
      ],
      "metadata": {
        "id": "oaqwFpfvLmuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -f train.csv\n",
        "!rm -f test.csv\n",
        "!wget https://www.dl.dropboxusercontent.com/s/rs7sqtb87m30o17/train.csv\n",
        "!wget https://www.dl.dropboxusercontent.com/s/fu4wa76kiwlby7u/test.csv"
      ],
      "metadata": {
        "id": "MUsr1KHQMV9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data"
      ],
      "metadata": {
        "id": "ULLi1pPQMa3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "KSICXwk9MiyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the numbers of samples in training/test"
      ],
      "metadata": {
        "id": "LMlaEANFM5KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(df):\n",
        "    print(df[\"CATEGORY\"].value_counts())\n",
        "\n",
        "get_stats(df_train)"
      ],
      "metadata": {
        "id": "RsvGGS3kM07M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2858b3-b13a-4676-8d23-151727f81e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b    4530\n",
            "e    4178\n",
            "t    1225\n",
            "m     739\n",
            "Name: CATEGORY, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_stats(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDoC6y-HNCdP",
        "outputId": "29665a82-4035-4995-ab0f-b127d2654825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b    558\n",
            "e    541\n",
            "t    155\n",
            "m     80\n",
            "Name: CATEGORY, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading data"
      ],
      "metadata": {
        "id": "JqPYGWmdNrg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = df_train['TITLE']\n",
        "y_train = df_train['CATEGORY']\n",
        "\n",
        "test_texts = df_test['TITLE']\n",
        "y_test = df_test['CATEGORY']"
      ],
      "metadata": {
        "id": "dN0C5Cf2Nu8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess data"
      ],
      "metadata": {
        "id": "1Gr9ikwzQYSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2TwSl4gQuqe",
        "outputId": "af84144b-897d-4b5b-8bb6-ec1992dfa6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.strip()\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    text  = \" \".join(tokens).lower()\n",
        "    return text\n",
        "\n",
        "print(train_texts[0])\n",
        "print(preprocess(train_texts[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ak13eJgQaBO",
        "outputId": "2388902e-b8ca-48e6-8aae-f03fff9eee7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taco Bell reveals 'secret' ingredients of mystery beef that's 88 per cent cow\n",
            "taco bell reveals 'secret ' ingredients of mystery beef that 's 88 per cent cow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_clean_texts = [preprocess(t) for t in train_texts]\n",
        "test_clean_texts = [preprocess(t) for t in test_texts]"
      ],
      "metadata": {
        "id": "LFpAw3gFPgpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a logistic regression model with BoW features"
      ],
      "metadata": {
        "id": "LSEt2cWmNTc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=20000)\n",
        "X_train = vectorizer.fit_transform(train_clean_texts)\n",
        "X_test = vectorizer.transform(test_clean_texts)\n",
        "\n",
        "clf = LogisticRegression(max_iter=500)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_preds = clf.predict(X_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzcp3pljNY3b",
        "outputId": "1455858f-206e-4a2f-e282-76dcbac8cd67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           b       0.91      0.96      0.94       558\n",
            "           e       0.93      0.97      0.95       541\n",
            "           m       0.90      0.66      0.76        80\n",
            "           t       0.89      0.71      0.79       155\n",
            "\n",
            "    accuracy                           0.92      1334\n",
            "   macro avg       0.91      0.83      0.86      1334\n",
            "weighted avg       0.92      0.92      0.91      1334\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using averaged features derived from pre-trained word embeddings\n",
        "\n",
        "We will calculate the average of word vectors in a sentence"
      ],
      "metadata": {
        "id": "5aFG7H-ZOhgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sent2vec(s):\n",
        "    \"\"\"Get the feature vector of a sentence\n",
        "    \"\"\"\n",
        "    words = s.split()\n",
        "    list_of_vectors = [wv[w] for w in words if w in wv]\n",
        "    list_of_vectors = np.array(list_of_vectors, dtype=object)\n",
        "\n",
        "    return np.mean(list_of_vectors, axis=0)\n",
        "\n",
        "X_train_w2v = np.array([sent2vec(s) for s in train_clean_texts], dtype=object)\n",
        "X_test_w2v = np.array([sent2vec(s) for s in test_clean_texts], dtype=object)\n",
        "\n",
        "X_train_w2v.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKlbxLOePIMH",
        "outputId": "c57a7a09-b87c-46dd-9466-4b9ef3190a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10672, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Logistic Regression with Word Embedding Features"
      ],
      "metadata": {
        "id": "LthE7c9Whgwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(max_iter=500)\n",
        "clf.fit(X_train_w2v, y_train)\n",
        "\n",
        "y_preds = clf.predict(X_test_w2v)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4VSHa1jhjjg",
        "outputId": "5ee27399-e239-4919-fbbd-598a1274a731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           b       0.89      0.94      0.91       558\n",
            "           e       0.94      0.96      0.95       541\n",
            "           m       0.86      0.82      0.84        80\n",
            "           t       0.84      0.63      0.72       155\n",
            "\n",
            "    accuracy                           0.90      1334\n",
            "   macro avg       0.88      0.84      0.86      1334\n",
            "weighted avg       0.90      0.90      0.90      1334\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "- [Word2Vec Model Tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py) on gensim documentation"
      ],
      "metadata": {
        "id": "hUQIMRBvFYA8"
      }
    }
  ]
}