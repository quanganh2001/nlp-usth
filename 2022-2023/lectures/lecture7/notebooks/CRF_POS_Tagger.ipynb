{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V63IQ6FIPLyy"
      },
      "source": [
        "# CRF model for POS Tagging\n",
        "\n",
        "In this tutorial we are going to use [python-crfsuite package](https://github.com/scrapinghub/python-crfsuite) for training a CRF Model for POS tagging problem. The method we introduced here can be applied to other tagging problems such as Word Segmentation, NER, NP Chunking, and so on.\n",
        "\n",
        "We will use the same dataset that we used for implementing HMM POS tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## seqeval"
      ],
      "metadata": {
        "id": "JuPQlLCeYOEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q seqeval[cpu]"
      ],
      "metadata": {
        "id": "gY5IC2NAYPL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxRu4etd5qAC"
      },
      "source": [
        "## python-crfsuite\n",
        "\n",
        "[python-crfsuite](https://github.com/scrapinghub/python-crfsuite) is a python binding to CRFsuite.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-MNLi-19co8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6bb08d-6f49-4b17-89ca-1f0dfc5b4632"
      },
      "source": [
        "!pip install -q python-crfsuite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/993.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6QuZOSbc46u"
      },
      "source": [
        "We import necessary packages for our work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wt4t2Ifc7ZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2583f80f-046f-4bee-82f4-7e280b87a809"
      },
      "source": [
        "from itertools import chain\n",
        "import nltk\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import sklearn\n",
        "import pycrfsuite\n",
        "import scipy.stats\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n68YiENfVSLh"
      },
      "source": [
        "## Loading data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBfD9tM9Vl3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df38c2e1-16c6-47f7-9155-9621c4702e2a"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('treebank')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create train/test/split"
      ],
      "metadata": {
        "id": "fFk2GuhjWg_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tagged_sentences = treebank.tagged_sents(tagset='universal')\n",
        "train_tagged_sentences, test_tagged_sentences = train_test_split(tagged_sentences, test_size=0.2, random_state=42)\n",
        "\n",
        "train_sentences = []\n",
        "train_tag_sequences = []\n",
        "\n",
        "test_sentences = []\n",
        "test_tag_sequences = []\n",
        "\n",
        "for sen in test_tagged_sentences:\n",
        "    words, tags = zip(*sen)\n",
        "    test_sentences.append(words)\n",
        "    test_tag_sequences.append(tags)\n",
        "\n",
        "for sen in train_tagged_sentences:\n",
        "    words, tags = zip(*sen)\n",
        "    train_sentences.append(words)\n",
        "    train_tag_sequences.append(tags)"
      ],
      "metadata": {
        "id": "Qu45VLPTAuEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QzzdaF49_JD",
        "outputId": "327d420b-495f-4039-d987-6282bf0b5f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Pierre',\n",
              " 'Vinken',\n",
              " ',',\n",
              " '61',\n",
              " 'years',\n",
              " 'old',\n",
              " ',',\n",
              " 'will',\n",
              " 'join',\n",
              " 'the',\n",
              " 'board',\n",
              " 'as',\n",
              " 'a',\n",
              " 'nonexecutive',\n",
              " 'director',\n",
              " 'Nov.',\n",
              " '29',\n",
              " '.')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tag_sequences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf-dgK2m-Dxs",
        "outputId": "cf92362a-814c-4113-9596-a469a041caeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('NOUN',\n",
              " 'NOUN',\n",
              " '.',\n",
              " 'NUM',\n",
              " 'NOUN',\n",
              " 'ADJ',\n",
              " '.',\n",
              " 'VERB',\n",
              " 'VERB',\n",
              " 'DET',\n",
              " 'NOUN',\n",
              " 'ADP',\n",
              " 'DET',\n",
              " 'ADJ',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'NUM',\n",
              " '.')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN11qSEXWx4H"
      },
      "source": [
        "## Features\n",
        "\n",
        "Next, define some features. In this example we use the list of features introducted the tutorial in [https://medium.com/analytics-vidhya/pos-tagging-using-conditional-random-fields-92077e5eaa31](https://medium.com/analytics-vidhya/pos-tagging-using-conditional-random-fields-92077e5eaa31).\n",
        "\n",
        "We will write a function that returns a dictionary of following features for each word in the sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLHKosUlXvDl"
      },
      "source": [
        "def is_all_caps(word):\n",
        "    return word.upper() == word and not word.isdigit()\n",
        "\n",
        "def word2features(sentence, i):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        sentence (list): list of words [w1, w2,...,w_n]\n",
        "        i (int): index of the word\n",
        "    Return:\n",
        "        features (dict): dictionary of features\n",
        "    \"\"\"\n",
        "    word = sentence[i]\n",
        "    features = {\n",
        "        'is_first': i == 0,\n",
        "        'is_last': i == len(sentence) - 1,\n",
        "        'is_first_capital': word[0].isupper(),\n",
        "        'is_all_caps': is_all_caps(word),    # ????\n",
        "        'is_all_lower': word.lower() == word,  # ????\n",
        "        'word': word,\n",
        "        'word.lower()': word.lower(),\n",
        "        'prefix_1': word[0],\n",
        "        'prefix_2': word[:2],\n",
        "        'prefix_3': word[:3],\n",
        "        'prefix_4': word[:4],\n",
        "        'suffix_1': word[-1],\n",
        "        'suffix_2': word[-2:],\n",
        "        'suffix_3': word[-3:],\n",
        "        'suffix_4': word[-4:],\n",
        "        'prev_word': '' if i==0 else sentence[i-1].lower(),\n",
        "        'next_word': '' if i==len(sentence)-1 else sentence[i+1].lower(),\n",
        "        'has_hyphen': '-' in word,\n",
        "        'is_numeric': word.isdigit(),\n",
        "        'capitals_inside': word[1:].lower() != word[1:]    # ????\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of words [w1, w2,...,w_n]\n",
        "    \"\"\"\n",
        "    return [word2features(sentence, i) for i in range(len(sentence))]\n",
        "\n",
        "\n",
        "def sent2labels(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of tuples (word, postag)\n",
        "    \"\"\"\n",
        "    return [postag for token, postag in sentence]\n",
        "\n",
        "def untag(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of tuples (word, postag)\n",
        "    \"\"\"\n",
        "    return [token for token, _ in sentence]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIoNQtI3a3CU"
      },
      "source": [
        "Let's see how the feature function works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKFTv1AJbAwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470c561c-1ac7-4f8e-8bc1-17ab82bf7083"
      },
      "source": [
        "sent2features( train_sentences[0] )[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is_first': False,\n",
              " 'is_last': False,\n",
              " 'is_first_capital': False,\n",
              " 'is_all_caps': False,\n",
              " 'is_all_lower': True,\n",
              " 'word': 'board',\n",
              " 'word.lower()': 'board',\n",
              " 'prefix_1': 'b',\n",
              " 'prefix_2': 'bo',\n",
              " 'prefix_3': 'boa',\n",
              " 'prefix_4': 'boar',\n",
              " 'suffix_1': 'd',\n",
              " 'suffix_2': 'rd',\n",
              " 'suffix_3': 'ard',\n",
              " 'suffix_4': 'oard',\n",
              " 'prev_word': 'the',\n",
              " 'next_word': 'as',\n",
              " 'has_hyphen': False,\n",
              " 'is_numeric': False,\n",
              " 'capitals_inside': False}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1hrd0lsbM0g"
      },
      "source": [
        "Now we can extract features from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDiZFM_ZcMiq"
      },
      "source": [
        "X_train = [sent2features(s) for s in train_sentences]\n",
        "y_train = train_tag_sequences\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sentences]\n",
        "y_test = test_tag_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bcy5mRVccL5"
      },
      "source": [
        "## Training\n",
        "\n",
        "To see all possible CRF parameters check its docstring. Here we are using SGD training algorithm with L2 regularization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pycrfsuite.Trainer(algorithm='lbfgs', verbose=True)\n",
        "\n",
        "for xseq, yseq in zip(X_train, y_train):\n",
        "    trainer.append(xseq, yseq)"
      ],
      "metadata": {
        "id": "0W5c_qPtXTgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WCAKroPXiLO",
        "outputId": "e01baa0b-f2b5-4ec1-bd09-bdc9ba7d7223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feature.minfreq',\n",
              " 'feature.possible_states',\n",
              " 'feature.possible_transitions',\n",
              " 'c1',\n",
              " 'c2',\n",
              " 'max_iterations',\n",
              " 'num_memories',\n",
              " 'epsilon',\n",
              " 'period',\n",
              " 'delta',\n",
              " 'linesearch',\n",
              " 'max_linesearch']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.set_params({\n",
        "    'c1': 1.0,   # coefficient for L1 penalty\n",
        "    'c2': 1e-3,  # coefficient for L2 penalty\n",
        "    'max_iterations': 50,  # stop earlier\n",
        "\n",
        "    # include transitions that are possible, but not observed\n",
        "    'feature.possible_transitions': True\n",
        "})"
      ],
      "metadata": {
        "id": "aBYyFX-iWVhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainer.train('postagger.crfsuite')"
      ],
      "metadata": {
        "id": "BnpH2j3aXvg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByTdDZj7dhaY"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Now we will evaluate our trained CRF model on the test data. We will use accuracy as our evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0oDIb7Fd6GH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c19e83-91c3-4e7a-ac5e-de614801b4c2"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "tagger = pycrfsuite.Tagger()\n",
        "tagger.open('postagger.crfsuite')\n",
        "\n",
        "y_pred = list( chain(*[tagger.tag(xseq) for xseq in X_test]) )\n",
        "y_true = list( chain(*y_test) )\n",
        "\n",
        "print(accuracy_score(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9726994014307265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHMCLo0n6tPZ"
      },
      "source": [
        "We obtained much better result than that of the first-order HMM model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXCaqzWKd78N"
      },
      "source": [
        "Let's see the details of classification results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGMmpyRfa91B",
        "outputId": "9073d924-a3c5-4155-9501-8969832e348d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           .       1.00      1.00      1.00      2354\n",
            "         ADJ       0.91      0.87      0.89      1316\n",
            "         ADP       0.98      0.99      0.98      2028\n",
            "         ADV       0.91      0.92      0.92       634\n",
            "        CONJ       1.00      0.99      0.99       471\n",
            "         DET       0.99      0.99      0.99      1795\n",
            "        NOUN       0.96      0.98      0.97      5943\n",
            "         NUM       1.00      0.99      0.99       727\n",
            "        PRON       0.99      1.00      1.00       523\n",
            "         PRT       0.98      0.98      0.98       658\n",
            "        VERB       0.96      0.96      0.96      2740\n",
            "           X       1.00      1.00      1.00      1360\n",
            "\n",
            "    accuracy                           0.97     20549\n",
            "   macro avg       0.97      0.97      0.97     20549\n",
            "weighted avg       0.97      0.97      0.97     20549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1epdvM8JpDB4"
      },
      "source": [
        "## Let’s check what classifier learned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh-wtda5pF-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa3000c-054b-4146-b027-076ff12f86c1"
      },
      "source": [
        "from collections import Counter\n",
        "info = tagger.info()\n",
        "\n",
        "def print_transitions(trans_features):\n",
        "    for (label_from, label_to), weight in trans_features:\n",
        "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
        "\n",
        "print(\"Top likely transitions:\")\n",
        "print_transitions(Counter(info.transitions).most_common(15))\n",
        "\n",
        "print(\"\\nTop unlikely transitions:\")\n",
        "print_transitions(Counter(info.transitions).most_common()[-15:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top likely transitions:\n",
            "ADJ    -> NOUN    2.829943\n",
            "VERB   -> PRT     2.367747\n",
            "X      -> VERB    1.617303\n",
            "NOUN   -> PRT     1.517131\n",
            "ADP    -> NOUN    1.479857\n",
            "ADP    -> PRON    1.470597\n",
            "DET    -> NOUN    1.467798\n",
            "ADV    -> ADJ     1.454885\n",
            "ADV    -> ADV     1.416952\n",
            "NUM    -> NOUN    1.414520\n",
            "ADV    -> VERB    1.356902\n",
            "DET    -> X       1.315338\n",
            "NOUN   -> VERB    1.305948\n",
            "NOUN   -> NOUN    1.288232\n",
            "ADP    -> DET     1.266361\n",
            "\n",
            "Top unlikely transitions:\n",
            "PRT    -> .       -0.960814\n",
            "ADJ    -> PRON    -0.971327\n",
            "PRON   -> DET     -0.972367\n",
            "DET    -> .       -1.071193\n",
            "PRT    -> PRT     -1.112169\n",
            "ADJ    -> DET     -1.214018\n",
            "PRT    -> NUM     -1.234834\n",
            "X      -> NOUN    -1.239027\n",
            "CONJ   -> .       -1.293765\n",
            "DET    -> ADP     -1.357986\n",
            "X      -> PRT     -1.606091\n",
            "ADP    -> X       -2.647249\n",
            "CONJ   -> X       -2.761592\n",
            ".      -> PRT     -3.404648\n",
            "DET    -> PRT     -3.933182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "pG1djHihrKtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sen = ['The', 'market', 'is', 'just', 'becoming', 'more', 'efficient', '.', \"''\"]\n",
        "tagger.tag(sent2features( sen ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP2r1LglrMBO",
        "outputId": "cb6524cc-eb53-4121-f6c2-36e3ce9f1f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DET', 'NOUN', 'VERB', 'ADV', 'VERB', 'ADV', 'ADJ', '.', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVjnzZq6ojoE"
      },
      "source": [
        "## References\n",
        "\n",
        "- [sklearn-crfsuite tutorial](https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-use-conll-2002-data-to-build-a-ner-system).\n",
        "- [Quick Recipe: Build a POS tagger using a Conditional Random Field](https://nlpforhackers.io/crf-pos-tagger/)\n",
        "- [NLP Guide: Identifying Part of Speech Tags using Conditional Random Fields](https://medium.com/analytics-vidhya/pos-tagging-using-conditional-random-fields-92077e5eaa31)\n",
        "- [python-crfsuite](https://github.com/scrapinghub/python-crfsuite)"
      ]
    }
  ]
}