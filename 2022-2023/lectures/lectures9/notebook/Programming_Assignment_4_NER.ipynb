{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition with MIT Restaurant Dataset\n",
        "\n",
        "Your name: Nguyen Van A\n",
        "\n",
        "Student ID: USTH001\n",
        "\n",
        "**Due: 23:59 19/3/2023**\n",
        "\n",
        "## Task Description\n",
        "\n",
        "In this assignment, you will train a NER Model using Conditional Random Fields (CRF) on and report the accuracy of your model on the test dataset.\n",
        "\n",
        "You will use the [MIT Restaurant Dataset](https://groups.csail.mit.edu/sls/downloads/restaurant/) dataset to do the task.\n",
        "\n",
        "## How to submit\n",
        "\n",
        "- Attach notebook file (.ipynb) and submit your work to Google Class Room\n",
        "- Name your file as YourName_StudentID_Assignment4.ibynb. E.g., Nguyen_Van_A_ST099834_Assignment4.ipynb\n",
        "- Write your name and student ID into this notebook\n",
        "- Copying others' assignments is strictly prohibited.\n"
      ],
      "metadata": {
        "id": "PGcNXgEH9S3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install python-crfsuite"
      ],
      "metadata": {
        "id": "7-wE5Ygy_851"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-crfsuite"
      ],
      "metadata": {
        "id": "qhjb-9J2AAMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde9e98e-5a95-4f11-bb28-e0891e92d91c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 163 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 245 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 266 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 276 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 286 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 296 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 481 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 491 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 501 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 522 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 532 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 542 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 552 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 563 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 573 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 583 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 593 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 716 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 727 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 737 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 743 kB 18.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "26ieJ1aEAQs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "import pycrfsuite"
      ],
      "metadata": {
        "id": "yGewX5VmASAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "We will use [MIT Restaurant Dataset](https://groups.csail.mit.edu/sls/downloads/restaurant/) dataset.\n",
        "\n",
        "The data set is already in CoNLL format. We will use the [train](https://groups.csail.mit.edu/sls/downloads/restaurant/restauranttrain.bio) data to create the NER model and evaluate the model on the [test](https://groups.csail.mit.edu/sls/downloads/restaurant/restauranttest.bio) data."
      ],
      "metadata": {
        "id": "Ie-POGzEv8xA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download data"
      ],
      "metadata": {
        "id": "2U0ME8SbxIS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -f restauranttrain.bio\n",
        "!rm -f restauranttest.bio\n",
        "\n",
        "!wget https://groups.csail.mit.edu/sls/downloads/restaurant/restauranttest.bio\n",
        "!wget https://groups.csail.mit.edu/sls/downloads/restaurant/restauranttrain.bio"
      ],
      "metadata": {
        "id": "PcFJBAy7xa-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data (30 points)\n",
        "\n",
        "In this part, you will load a data file into a list of sentences. Each sentence is a list of (word, tag) tuples.\n",
        "\n",
        "**Note: Blank lines are used to seperate sentences.**\n",
        "\n",
        "For instance, the sentence below will be loaded into a list\n",
        "\n",
        "```\n",
        "O\ta\n",
        "B-Rating\tfour\n",
        "I-Rating\tstar\n",
        "O\trestaurant\n",
        "B-Location\twith\n",
        "I-Location\ta\n",
        "B-Amenity\tbar\n",
        "```\n",
        "\n",
        "You will complete the function below"
      ],
      "metadata": {
        "id": "MLyiSFarytjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Add necessary import here\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load data into a list of list of (word, tag) tuples\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to data\n",
        "\n",
        "    Returns:\n",
        "        sentences: list of (word, tag) tuples\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "\n",
        "    #TODO: Write your code here\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "M6UmjUBP06c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sents = load_data('restauranttrain.bio')\n",
        "test_sents = load_data('restauranttest.bio')"
      ],
      "metadata": {
        "id": "zy6fIDnF19nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the number of sentences in train and test data"
      ],
      "metadata": {
        "id": "oOzFyaKo2Wgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_sents)"
      ],
      "metadata": {
        "id": "jVdNqwKH2mi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_sents)"
      ],
      "metadata": {
        "id": "sVVcSv6r2oq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sents[0]"
      ],
      "metadata": {
        "id": "Msy21RWM232U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Features (50 points)\n",
        "\n",
        "We can extract as many features as you want. You will implement following basic features.\n",
        "\n",
        "※ Of course, you can add more features.\n",
        "\n",
        "*Word identity (lowercase)*\n",
        "\n",
        "- Previous word identity\n",
        "- Current word identity\n",
        "- Next word\n",
        "- Previous word and current word combination. Concat the previous word the current word by '||'\n",
        "- Current word and next word combination. Concat two words by '||'\n",
        "\n",
        "*Word shapes*\n",
        "\n",
        "- Word prefix and suffix (4 characters)\n",
        "- The first character of the current word is the capital letter\n",
        "\n",
        "**All you need to do is to complete the function `word2feature`.**"
      ],
      "metadata": {
        "id": "FVv2iYez3CBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word2features(sentence, i):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        sentence (list): list of words [w1, w2,...,w_n]\n",
        "        i (int): index of the word\n",
        "    Return:\n",
        "        features (dict): dictionary of features\n",
        "    \"\"\"\n",
        "    word = sentence[i]\n",
        "    prev_word = '' if i==0 else sentence[i-1].lower()\n",
        "    next_word = '' if i==len(sentence)-1 else sentence[i+1].lower()\n",
        "    features = {\n",
        "        #TODO: Write your features here\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of words [w1, w2,...,w_n]\n",
        "    \"\"\"\n",
        "    return [word2features(sentence, i) for i in range(len(sentence))]\n",
        "\n",
        "\n",
        "def sent2labels(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of tuples (word, postag)\n",
        "    \"\"\"\n",
        "    return [tag for token, tag in sentence]\n",
        "\n",
        "def untag(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of tuples (word, postag)\n",
        "    \"\"\"\n",
        "    return [token for token, _ in sentence]"
      ],
      "metadata": {
        "id": "yfBAsejb5iir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to extract features for the first sentence"
      ],
      "metadata": {
        "id": "t7fSWZJQ83Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sents[0]"
      ],
      "metadata": {
        "id": "iNrn8zBrS-Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent2features(untag(train_sents[0]))[0]"
      ],
      "metadata": {
        "id": "KFTpTYO68tfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create train/test data"
      ],
      "metadata": {
        "id": "9Xl5Tyd4-zZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [sent2features(untag(s)) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_test = [sent2features(untag(s)) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]"
      ],
      "metadata": {
        "id": "nKUVYzMQ-3ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "La08Oyam_ZjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainer = pycrfsuite.Trainer(verbose=False)\n",
        "\n",
        "for xseq, yseq in zip(X_train, y_train):\n",
        "    trainer.append(xseq, yseq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-C-fq8x_brV",
        "outputId": "5bd7d083-2533-46fe-d5e2-810793f5a37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.01 s, sys: 8.76 ms, total: 1.02 s\n",
            "Wall time: 1.03 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set model parameters\n",
        "\n",
        "max_iterations = 50 #@param[50, 20, 100]\n",
        "\n",
        "trainer.set_params({\n",
        "    'c1': 1.0,   # coefficient for L1 penalty\n",
        "    'c2': 1e-3,  # coefficient for L2 penalty\n",
        "    'max_iterations': max_iterations,\n",
        "\n",
        "    # include transitions that are possible, but not observed\n",
        "    'feature.possible_transitions': True\n",
        "})"
      ],
      "metadata": {
        "id": "C0XJtpOtO2Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainer.train('mitrestaurant.crfsuite')"
      ],
      "metadata": {
        "id": "v5VXTKZdO4aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation (20 points)\n",
        "\n",
        "We will use [seqeval](https://github.com/chakki-works/seqeval) package for evaluation NER result."
      ],
      "metadata": {
        "id": "wQQLY7m-PPmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q seqeval[cpu]"
      ],
      "metadata": {
        "id": "GtpO8Mb4dFMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Predictions"
      ],
      "metadata": {
        "id": "6UCqdlNvPWZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagger = pycrfsuite.Tagger()\n",
        "tagger.open('mitrestaurant.crfsuite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVenrYzcPYoM",
        "outputId": "6f31448d-f053-4b13-da56-9160a5fa9bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.closing at 0x7fa0a3ce0c50>"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_sent = test_sents[0]\n",
        "example_sent"
      ],
      "metadata": {
        "id": "rAfV8QhcPb9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(untag(example_sent)))))\n",
        "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))"
      ],
      "metadata": {
        "id": "8hGrDn-4P55R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y_pred = [tagger.tag(xseq) for xseq in X_test]"
      ],
      "metadata": {
        "id": "3ee18ZKIQqgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Ns2UeFU5U96O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Datasets for Entity Recognition: https://github.com/juand-r/entity-recognition-datasets\n",
        "2. [sklearn-crfsuite tutorial](https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-use-conll-2002-data-to-build-a-ner-system).\n",
        "3. [Quick Recipe: Build a POS tagger using a Conditional Random Field](https://nlpforhackers.io/crf-pos-tagger/)\n",
        "4. [NLP Guide: Identifying Part of Speech Tags using Conditional Random Fields](https://medium.com/analytics-vidhya/pos-tagging-using-conditional-random-fields-92077e5eaa31)\n",
        "5. [CRFsuite - Tutorial on Chunking Task](http://www.chokkan.org/software/crfsuite/tutorial.html)"
      ],
      "metadata": {
        "id": "oRMhFCCEBhds"
      }
    }
  ]
}