{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started with Text Analysis\n",
        "\n",
        "NLP Course 2022\n",
        "\n",
        "## Objectives\n",
        "\n",
        "In this documents, we will learn following text analysis techniques:\n",
        "\n",
        "- How to clean text data with regular expression\n",
        "-Â Tokenisation\n",
        "- POS Tagging\n",
        "\n",
        "We will see how to use NLP Toolkits such as NLTK, SpaCY, Underthesea, PhoNLP to do NLP pipeline."
      ],
      "metadata": {
        "id": "yscsqVusJ07P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Texts\n",
        "\n",
        "In NLP tasks, we may want to remove irrelevant contents in input data before using that for training or prediction. For instance, when we will delete URLs, punctuations in Twitter Posts in sentiment analysis task. We can do that with [Regular Expressions](https://www.tutorialspoint.com/python/python_reg_expressions.htm).\n",
        "\n",
        "In Python we can use `re` module for regular expressions.\n"
      ],
      "metadata": {
        "id": "7TohQGX21Luo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo data"
      ],
      "metadata": {
        "id": "3U5H216N3H5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = \"Stories of Pittsburghers trapped in #Houston flooding!!!! @@ - https://t.co/j5igfpvLJu https://t.co/8gsUpD8jsa\""
      ],
      "metadata": {
        "id": "NkX8DVKn3OCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove URL"
      ],
      "metadata": {
        "id": "Wpx-B9Xf3P7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# When the UNICODE flag is not specified, matches any non-whitespace character\n",
        "result = re.sub(r\"[(http(s)?):\\/\\/(www\\.)?a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", sent1)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hnFbggJZ3ZhK",
        "outputId": "5b296b7e-cd09-43d4-fce6-e3efc9c95dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Stories of Pittsburghers trapped in #Houston flooding!!!! @@ -  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove punctuations in text"
      ],
      "metadata": {
        "id": "Qsl6K6Bf3uez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "xcnKBU5j34k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = str.maketrans('', '', string.punctuation)\n",
        "result2 = result.translate(translator)\n",
        "result2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oFTtImLc4Phf",
        "outputId": "3d941f73-b48c-4376-e638-64b926374a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Stories of Pittsburghers trapped in Houston flooding    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NTLK (Natural Language Toolkit)\n",
        "\n",
        "[NLTK](https://www.nltk.org/) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
        "\n",
        "We will see how to use nltk to do basic text analysis in NLP pipeline.\n"
      ],
      "metadata": {
        "id": "YN_lo5I74nfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence tokenization\n",
        "\n",
        "We split a long paragraph into sentences."
      ],
      "metadata": {
        "id": "i0OvmkVY8D7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"Hello World Dr. John. It's good to see you. Thanks for buying this book.\""
      ],
      "metadata": {
        "id": "lSzHbdqV8NuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to download appropriate models before using nltk to do some tasks."
      ],
      "metadata": {
        "id": "_daqy3TC8ZLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJJuV9uN8lOu",
        "outputId": "d50a3716-f71e-45d4-8d10-a047b9fd5a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(para)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuxa9LFA9cwi",
        "outputId": "69edddca-20c9-496d-bb68-02ad512e20db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello World Dr. John.',\n",
              " \"It's good to see you.\",\n",
              " 'Thanks for buying this book.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word tokenization\n",
        "\n",
        "Word tokenization is to split a sentence into tokens."
      ],
      "metadata": {
        "id": "FiMAjqaV9g2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sent = 'The history of NLP generally starts in the 1950s, although work can be found from earlier periods.'\n",
        "word_tokenize(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9ZEk8TRPvfk",
        "outputId": "81cdc4f6-95ef-4c55-8031-a817b20fe7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'history',\n",
              " 'of',\n",
              " 'NLP',\n",
              " 'generally',\n",
              " 'starts',\n",
              " 'in',\n",
              " 'the',\n",
              " '1950s',\n",
              " ',',\n",
              " 'although',\n",
              " 'work',\n",
              " 'can',\n",
              " 'be',\n",
              " 'found',\n",
              " 'from',\n",
              " 'earlier',\n",
              " 'periods',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS Tagging\n",
        "\n",
        "POS Tagging is the process of assigning Part-of-speech to each token in a sentence. We need to tokenize the sentence first before performing POS Tagging.\n",
        "\n",
        "In NLTK, we need to download the tagging model first."
      ],
      "metadata": {
        "id": "FGtoMVgMPxfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FWKGy8eReR-",
        "outputId": "567af999-002b-4bc9-9e36-f25d9bd31e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sent = 'The history of NLP generally starts in the 1950s, although work can be found from earlier periods.'\n",
        "pos_tag(word_tokenize(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmu6kLiUQWOh",
        "outputId": "58a1e06a-6866-490d-fcdd-2bff652df8b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('history', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('NLP', 'NNP'),\n",
              " ('generally', 'RB'),\n",
              " ('starts', 'VBZ'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('1950s', 'CD'),\n",
              " (',', ','),\n",
              " ('although', 'IN'),\n",
              " ('work', 'NN'),\n",
              " ('can', 'MD'),\n",
              " ('be', 'VB'),\n",
              " ('found', 'VBN'),\n",
              " ('from', 'IN'),\n",
              " ('earlier', 'JJR'),\n",
              " ('periods', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering stop words\n",
        "\n",
        "What are stopwords?\n",
        "\n",
        "Stop words are the words in a stop list (or stoplist or negative dictionary) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant. (Wikipedia).\n",
        "\n",
        "We are going to filter out stop words in a sentence.\n",
        "\n",
        "NLTK includes English stop words."
      ],
      "metadata": {
        "id": "YFcdFb__Q_z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqX8OJXfS2VZ",
        "outputId": "a96dfab4-5a7c-4c8b-9094-6690f9e3f496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "english_stops = set(stopwords.words('english'))\n",
        "\n",
        "sent = 'The history of NLP generally starts in the 1950s, although work can be found from earlier periods.'\n",
        "\n",
        "words = nltk.word_tokenize(sent)\n",
        "words_without_stopwords = [word for word in words if word not in english_stops]\n",
        "words_without_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRG4SSw5Tsq8",
        "outputId": "f61b02be-4fd1-49ab-d9ce-03bda26a8c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'history',\n",
              " 'NLP',\n",
              " 'generally',\n",
              " 'starts',\n",
              " '1950s',\n",
              " ',',\n",
              " 'although',\n",
              " 'work',\n",
              " 'found',\n",
              " 'earlier',\n",
              " 'periods',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('french')"
      ],
      "metadata": {
        "id": "IMymSDS9KuAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## spaCy\n",
        "\n",
        "[spaCy](https://spacy.io/) is an open-source software library for advanced natural language processing, written in Python and Cython. Let's see how we can do NLP Pipeline with spaCy.\n",
        "\n",
        "We need to download English model before using spaCy.\n"
      ],
      "metadata": {
        "id": "egBk-N64T4Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "jouE8yhnXGt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "MWrMwbLccobM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence tokenization"
      ],
      "metadata": {
        "id": "mLfKCmubWuy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"Hello World Dr. John. It's good to see you. Thanks for buying this book.\"\n",
        "doc = nlp(para)\n",
        "sents = [sent.text for sent in doc.sents]\n",
        "print(sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymVhbDsrciFD",
        "outputId": "11cc3e75-5407-448b-eb5f-d365e30d2f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello World Dr. John.', \"It's good to see you.\", 'Thanks for buying this book.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word tokenization"
      ],
      "metadata": {
        "id": "brypQEi1W1jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'The history of NLP generally starts in the 1950s, although work can be found from earlier periods.'\n",
        "doc = nlp(sent)\n",
        "tokens = [x.text for x in doc]\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCeH_cJXcszT",
        "outputId": "c76e0907-9e5c-410a-cd4b-c7aa691a5af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'history',\n",
              " 'of',\n",
              " 'NLP',\n",
              " 'generally',\n",
              " 'starts',\n",
              " 'in',\n",
              " 'the',\n",
              " '1950s',\n",
              " ',',\n",
              " 'although',\n",
              " 'work',\n",
              " 'can',\n",
              " 'be',\n",
              " 'found',\n",
              " 'from',\n",
              " 'earlier',\n",
              " 'periods',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS Tagging"
      ],
      "metadata": {
        "id": "Q862p1aWW6X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(x.text, x.pos_) for x in doc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_Hu6FcUdChy",
        "outputId": "3bf7ada2-533d-4a7c-e01d-045aed1ce1f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DET'),\n",
              " ('history', 'NOUN'),\n",
              " ('of', 'ADP'),\n",
              " ('NLP', 'PROPN'),\n",
              " ('generally', 'ADV'),\n",
              " ('starts', 'VERB'),\n",
              " ('in', 'ADP'),\n",
              " ('the', 'DET'),\n",
              " ('1950s', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('although', 'SCONJ'),\n",
              " ('work', 'NOUN'),\n",
              " ('can', 'AUX'),\n",
              " ('be', 'AUX'),\n",
              " ('found', 'VERB'),\n",
              " ('from', 'ADP'),\n",
              " ('earlier', 'ADJ'),\n",
              " ('periods', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering stop words"
      ],
      "metadata": {
        "id": "ylEbEtZJd2hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_stopwords = set(spacy.lang.en.stop_words.STOP_WORDS)\n",
        "sent = 'The history of NLP generally starts in the 1950s, although work can be found from earlier periods.'\n",
        "doc = nlp(sent)\n",
        "words = [x.text for x in doc]\n",
        "[word for word in words if word not in spacy_stopwords]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6IQWLrreWet",
        "outputId": "42eb7d4c-2358-4185-a036-3a7937e967dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'history',\n",
              " 'NLP',\n",
              " 'generally',\n",
              " 'starts',\n",
              " '1950s',\n",
              " ',',\n",
              " 'work',\n",
              " 'found',\n",
              " 'earlier',\n",
              " 'periods',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named-Entity Recognition"
      ],
      "metadata": {
        "id": "ovnyLt17W85N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\"Donald Trump has brought his drama show back to Washington early, \"\n",
        "        \"perhaps realizing his time in the White House is down to days and counting\")\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUnrQs43W_fe",
        "outputId": "c584ca3c-e4d0-4fda-ce99-89f3bd0770b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Donald Trump 0 12 PERSON\n",
            "Washington 48 58 GPE\n",
            "the White House 96 111 ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other NLP Toolkits for English\n",
        "\n",
        "- [Stanza](https://github.com/stanfordnlp/stanza/) (Python)\n",
        "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) (Java)\n",
        "- [Apache OpenNLP](https://github.com/apache/opennlp) (Java)\n",
        "- [DKPro Core](https://dkpro.github.io/dkpro-core/) (Java)"
      ],
      "metadata": {
        "id": "2qb2uhZQe-00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. [spaCy 101: Everything you need to know](https://spacy.io/usage/spacy-101)\n",
        "2. [Advanced NLP with spacy](https://course.spacy.io/)\n",
        "3. Bird, Steven; Klein, Ewan; Loper, Edward (2009). *Natural Language Processing with Python*. [http://www.nltk.org/book/](http://www.nltk.org/book/)\n",
        "4. [NLTK in 20 minutes](http://www.slideshare.net/japerk/nltk-in-20-minutes), by Jacob Perkins\n"
      ],
      "metadata": {
        "id": "KC7qd9Vfdk7u"
      }
    }
  ]
}